{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy \n",
    "import logging\n",
    "import psycopg2\n",
    "import itertools\n",
    "\n",
    "from collections import Counter\n",
    "from heapq import merge\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "from t1000.embedding import video\n",
    "\n",
    "def fetch(dbname, user, host, password, query):\n",
    "    '''\n",
    "    Executes query on a given database\n",
    "    '''\n",
    "    connection_string = \"dbname='{0}' user='{1}' host='{2}' password='{3}'\".format(\n",
    "        dbname, user, host, password)\n",
    "    try:\n",
    "        conn = psycopg2.connect(connection_string)\n",
    "        curr = conn.cursor()\n",
    "        curr.execute(query)\n",
    "        res = curr.fetchall()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logging.exception('')\n",
    "    finally:\n",
    "        curr.close()\n",
    "        conn.close()\n",
    "        \n",
    "    return res\n",
    "\n",
    "def inner_join(a, b):\n",
    "    '''\n",
    "    Joins two iterables of tuples on the first \n",
    "    element\n",
    "    \n",
    "    Arguments:\n",
    "    a - list of tuples (id, x)\n",
    "    b - list of tuples (id, y)\n",
    "    \n",
    "    Returns:\n",
    "    list of tuples (id, x, y)\n",
    "    '''\n",
    "    key = itemgetter(0)\n",
    "    a.sort(key=key) \n",
    "    b.sort(key=key)\n",
    "    for _, group in groupby(merge(a, b, key=key), key):\n",
    "        row_a, row_b = next(group), next(group, None)\n",
    "        if row_b is not None: # join\n",
    "            yield row_a + row_b[1:]\n",
    "            \n",
    "\n",
    "def filter_videos(videos, min_count = 10):\n",
    "    '''\n",
    "    Filters videos and returns mapping to the original tags\n",
    "    \n",
    "    Returns:\n",
    "    filtered       - a list with transformed and filtered videos\n",
    "    tags_2_indices - a dictinary that transforms tags to rank of their\n",
    "                     frequencies\n",
    "    indices_2_tags - inverse dictionary\n",
    "    '''\n",
    "    \n",
    "    # we have to iterate twice, first to create dictionary, then \n",
    "    # then to filter tags and transform the list\n",
    "    if not isinstance(videos, list):\n",
    "        videos = list(videos)\n",
    "        \n",
    "    # Filters top tags and creates mapping\n",
    "    count = Counter(itertools.chain(*[tup[1] for tup in videos]))\n",
    "    tags_2_indices = { \n",
    "        tag_id: index \n",
    "            for index, (tag_id, count) in enumerate(count.most_common(), 1)\n",
    "            if count >= min_count \n",
    "    }\n",
    "\n",
    "    # reverse index for decoding \n",
    "    indices_2_tags = { \n",
    "        v: k for k, v in tags_2_indices.items()\n",
    "    }\n",
    "\n",
    "    filtered = []\n",
    "    for video_id, tags, url in videos:\n",
    "        encoded = [tags_2_indices[t] for t in tags if t in tags_2_indices]\n",
    "        if encoded:\n",
    "            filtered.append((video_id, encoded, url))\n",
    "                \n",
    "    return filtered, tags_2_indices, indices_2_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbname='ds-wizards'\n",
    "user='wizard'\n",
    "host='192.95.32.117'\n",
    "password='GaG23jVxZhMnQaU53r8o'\n",
    "\n",
    "VQUERY = \"select post_id, url from videos where status='ok'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vres = fetch(dbname, user, host, password, VQUERY)\n",
    "vres = [(post_id.split(\"_\")[1], url) for post_id, url in vres]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbname='ds-content-tags'\n",
    "user='ds-content-tags'\n",
    "password='0fXjWl592vNf1gYvIw8w'\n",
    "host='192.95.32.117'\n",
    "\n",
    "TQUERY = \"select id, tags from videos where tags is not NULL\"\n",
    "TAGS = \"select tag_id, name, path from content_tags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tres = fetch(dbname, user, host, password, TQUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = { \n",
    "    tag_id: (name, path) for (tag_id, name, path) in fetch(\n",
    "        dbname, user, host, password, TAGS) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join videos with tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = inner_join(tres, vres)\n",
    "filtered, t2i, i2t = filter_videos(videos, 10)\n",
    "print(\"Found %d videos with %d unique tags\" % (len(filtered), len(t2i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from time import gmtime, strftime\n",
    "\n",
    "class FramesIterator:\n",
    "    '''iterator that yields raw frames from database'''\n",
    "\n",
    "    def __init__(self, videos):\n",
    "        self.videos = copy.deepcopy(videos)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.videos:\n",
    "            print(\"[%s]Downloading url\" % strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "            video_id, video_tags, video_path = self.videos.pop()\n",
    "            \n",
    "            # this could be done in parallel\n",
    "            frames = video.extract_frames(video_path)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        \n",
    "        return video_id, frames, video_tags\n",
    "\n",
    "\n",
    "def extract_incepction_v3(frame_iterator, model_dir, logging_step = 100):\n",
    "    '''\n",
    "    Extract incepction_v3 features from frame generator.\n",
    "    \n",
    "    Inputs:\n",
    "    frame_iterator - an iterator yielding video frames\n",
    "    model_dir      - a directory to inception model \n",
    "    logging_step   - log progress after this number of steps\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.debug(\"Extracting inception features\")\n",
    "\n",
    "    # load incepction 3 graph\n",
    "    with gfile.FastGFile(model_dir, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        _ = tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # TODO: add queuing and batching for optimal performance\n",
    "        for index, item in enumerate(frame_iterator):\n",
    "\n",
    "            if index % logging_step == 0:\n",
    "                print(\"Extracting features from video number %s\" % index)\n",
    "                logger.info(\"Extracting features from video number %s\" % index)\n",
    "            \n",
    "            item_id, frames, tags = item\n",
    "            img_features = []\n",
    "            for frame in frames:\n",
    "                # get tensor from network\n",
    "                pool3_layer = sess.graph.get_tensor_by_name('pool_3:0')\n",
    "                predictions = sess.run(pool3_layer, {'DecodeJpeg:0': frame})\n",
    "\n",
    "                # concatenate features\n",
    "                features = np.squeeze(predictions)\n",
    "                img_features.append(features)\n",
    "\n",
    "            np.array(img_features, dtype=np.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = FramesIterator(filtered[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_incepction_v3(frames, '/models/image/inception/classify_image_graph_def.pb',  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s  %(levelname)-8s %(message)s',\n",
    "                    datefmt='%m-%d %H:%M:%S')\n",
    "\n",
    "#Producer class\n",
    "class Producer(multiprocessing.Process):\n",
    "    def __init__(self, items, idx, queue):\n",
    "        super(producer, self).__init__()\n",
    "        self.items = items\n",
    "        self.queue = queue\n",
    "        self.idx = idx\n",
    "\n",
    "    def run(self):\n",
    "        logging.info(\"Starting %d producer \" % (self.idx ))\n",
    "        \n",
    "        while self.items:\n",
    "            item = self.items.pop()\n",
    "            self.queue.put(\"%d.%d\" % (self.idx, item))\n",
    "            time.sleep(2) #Unnecessary sleep to demonstrate order of events\n",
    "\n",
    "        logging.info(\"This is it! [%d]\" % self.idx)\n",
    "        self.queue.put(None)\n",
    "        \n",
    "        logging.info('Ending producer')\n",
    "        return\n",
    "\n",
    "#Consumer class\n",
    "class Consumer(multiprocessing.Process):\n",
    "    def __init__(self, idx, queues):\n",
    "        super(consumer, self).__init__()\n",
    "        self.queues = queues\n",
    "        self.idx = idx\n",
    "\n",
    "    def run(self):\n",
    "        logging.info(\"Starting %d consumer\" % (self.idx ))\n",
    "        while self.queues:\n",
    "            for queue in self.queues:\n",
    "                stuff = queue.get()\n",
    "                if stuff is None:\n",
    "                    self.queues[:] = [q for q in self.queues if q != queue]\n",
    "                    logging.info(\"Rmoved %s from queues. %d left\" % (queue, len(self.queues)))\n",
    "                    continue\n",
    "                \n",
    "                producer_id, value = stuff.split(\".\")\n",
    "                logging.info('Consumer %d: Got \"%s\" from %s' % (self.idx, value, producer_id))\n",
    "                time.sleep(1) #Unnecessary sleep to demonstrate order of events\n",
    "\n",
    "        logging.info(\"Ending %d consumer\" % (self.idx ))\n",
    "        return\n",
    "\n",
    "def chunks(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return out\n",
    "\n",
    "if __name__=='__main__':\n",
    "    NCORE = 4\n",
    "    NPRORD = 3\n",
    "    \n",
    "    work = list(range(10))\n",
    "    work = chunks(work, NPRORD)\n",
    "    \n",
    "\n",
    "    #make reader for reading data. lets call this object Producer\n",
    "    producers = []\n",
    "    queues = []\n",
    "    for idx in range(NPRORD):   \n",
    "        queues.append(multiprocessing.Queue())\n",
    "        producers.append(producer(work[idx], idx, queues[idx]))\n",
    "    \n",
    "    \n",
    "    q = multiprocessing.Queue()\n",
    "    queues.append(q)\n",
    "    producers.append(producer(list(range(20, 30)), 999, q))\n",
    "    \n",
    "    print(len(producers), len(queues))\n",
    "\n",
    "    #make receivers for the data. Lets call these Consumers\n",
    "    #Each consumer is assigned a queue\n",
    "    consumer_object = consumer(1, queues)\n",
    "    consumer_object.start()\n",
    "\n",
    "    # start the producer processes\n",
    "    for producer_object in producers:\n",
    "        producer_object.start()\n",
    "\n",
    "\n",
    "    #Join all started processes\n",
    "    consumer_object.join()    \n",
    "    \n",
    "    for producer_object in producers:\n",
    "        producer_object.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
